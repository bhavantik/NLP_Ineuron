{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 2\n",
    "1. Mention and define the NLTK steps.\n",
    "2. What different types of segmentation are there?\n",
    "3. Explain the method of recognising named entities.\n",
    "4. Make a list of the elements of NLP.\n",
    "5. Give an example of pragmatic analysis.\n",
    "6. Explain the morphological and lexical analysis procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mention and define the NLTK steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the Text Processing steps in NLTK:\n",
    "1. Tokenization\n",
    "2. Lower case conversion\n",
    "3. Stop Words removal\n",
    "4. Stemming\n",
    "5. Lemmatization\n",
    "6. Parse tree or Syntax Tree generation\n",
    "7. POS Tagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization\n",
    "The breaking down of text into smaller units is called tokens. tokens are a small part of that text. If we have a sentence, the idea is to separate each word and build a vocabulary such that we can represent all words uniquely in a list. Numbers, words, etc.. all fall under tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing is an exciting area.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Natural language processing is an exciting area.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'an', 'exciting', 'area', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lower case conversion\n",
    "- We want our model to not get confused by seeing the same word with different cases like one starting with capital and one without and interpret both differently. So we convert all words into the lower case to avoid redundancy in the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural', 'language', 'processing', 'is', 'an', 'exciting', 'area']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \",text.lower())\n",
    "words = text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Stop Words removal\n",
    "When we use the features from a text to model, we will encounter a lot of noise. These are the stop words like the, he, her, etc… which don’t help us and, just be removed before processing for cleaner processing inside the model. With NLTK we can see all the stop words available in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural', 'language', 'processing', 'exciting', 'area']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords fro text\n",
    "words_2 = []\n",
    "for i in words:\n",
    "    if i not in stopwords.words('english'):\n",
    "        words_2.append(i)\n",
    "        \n",
    "words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming\n",
    "In our text we may find many words like playing, played, playfully, etc… which have a root word, play all of these convey the same meaning. So we can just extract the root word and remove the rest. Here the root word formed is called ‘stem’ and it is not necessarily that stem needs to exist and have a meaning. Just by committing the suffix and prefix, we generate the stems.\n",
    "\n",
    "NLTK provides us with PorterStemmer LancasterStemmer and SnowballStemmer packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'excit', 'area']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words_2]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Lemmatization\n",
    "We want to extract the base form of the word here. The word extracted here is called Lemma and it is available in the dictionary. We have the WordNet corpus and the lemma generated will be available in this corpus. NLTK provides us with the WordNet Lemmatizer that makes use of the WordNet Database to lookup lemmas of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'is', 'an', 'exciting', 'area']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is much faster than lemmatization as it doesn’t need to lookup in the dictionary and just follows the algorithm to generate the root words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Parse tree or Syntax Tree generation\n",
    "We can define grammar and then use NLTK RegexpParser to extract all parts of speech from the sentence and draw functions to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "# Example text\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "# Find all parts of speech in above sentence\n",
    "tagged = pos_tag(word_tokenize(sample_text))\n",
    "#Extract all parts of speech from any text\n",
    "chunker = RegexpParser(\"\"\"\n",
    "NP: {?*} #To extract Noun Phrases\n",
    "P: {}  #To extract Prepositions\n",
    "V: {}  #To extract Verbs\n",
    "PP: {\n",
    "\n",
    "#To extract Prepositional Phrases\n",
    "VP: { *} #To extract Verb Phrases\n",
    "\"\"\")\n",
    "# Print all parts of speech in above sentence\n",
    "output = chunker.parse(tagged)\n",
    "print(\"After Extractingn\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax tree generation | NLTK\n",
    "output.draw()\n",
    "\n",
    "Source: https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. POS Tagging\n",
    "Part of Speech tagging is used in text processing to avoid confusion between two same words that have different meanings. With respect to the definition and context, we give each word a particular tag and process them. Two Steps are used here:\n",
    "\n",
    "Tokenize text (word_tokenize).\n",
    "Apply the pos_tag from NLTK to the above step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('exciting', 'JJ'), ('area', 'NN'), ('.', '.')]\n",
      "[('Huge', 'NNP'), ('budget', 'NN'), ('allocated', 'VBD'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"Natural language processing is an exciting area. Huge budget have been allocated for this.\"\n",
    "# sent_tokenize is one of instances of\n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "  # Word tokenizers is used to find the words\n",
    "  # and punctuation in a string\n",
    "  wordsList = nltk.word_tokenize(i)\n",
    "  # removing stop words from wordList\n",
    "  wordsList = [w for w in wordsList if not w in stop_words]\n",
    "  # Using a Tagger. Which is part-of-speech\n",
    "  # tagger or POS-tagger.\n",
    "  tagged = nltk.pos_tag(wordsList)\n",
    "  print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What different types of segmentation are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text segmentation is the task of dividing a document of text into coherent and semantically meaningful segments which are contiguous. This task is important for other Natural Language Processing (NLP) applications like summarization, context understanding, and question-answering.\n",
    "\n",
    "-  Word segmentation.\n",
    "-  Intent segmentation.\n",
    "-  Sentence segmentation.\n",
    "-  Topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explain the method of recognising named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can simple observed that after reading a particular text, naturally we can recognize named entities such as people, values, locations, and so on.\n",
    "\n",
    "For Example, Consider the following sentence:\n",
    "\n",
    "        \"Sentence: Sundar Pichai, the CEO of Google Inc. is walking in the streets of California.\"\n",
    "\n",
    "- From the above sentence, we can identify three types of entities: (Named Entities)\n",
    "\n",
    "        ( “person”: “Sundar Pichai” ),\n",
    "        (“org”: “Google Inc.”),\n",
    "        (“location”: “California”).\n",
    "- But to do the same thing with the help of computers, we need to help them recognize entities first so that they can categorize them. So, to do so we can take the help of machine learning and Natural Language Processing (NLP).\n",
    "\n",
    "- Let’s discuss the role of both these things while implementing NER using computers:\n",
    "\n",
    "- NLP: It studies the structure and rules of language and forms intelligent systems that are capable of deriving meaning from text and speech.\n",
    "- Machine Learning: It helps machines learn and improve over time.\n",
    "\n",
    "- To learn what an entity is, a NER model needs to be able to detect a word or string of words that form an entity (e.g. California) and decide which entity category it belongs to.\n",
    "\n",
    "- So, as a concluding step we can say that the heart of any NER model is a two-step process:\n",
    "    - Detect a named entity\n",
    "    - Categorize the entity\n",
    "- So first, we need to create entity categories, like Name, Location, Event, Organization, etc., and feed a NER model relevant training data.\n",
    "\n",
    "- Then, by tagging some samples of words and phrases with their corresponding entities, we’ll eventually teach our NER model to detect the entities and categorize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a list of the elements of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lexical Analysis and Morphological\n",
    "2. Syntactic Analysis (Parsing)\n",
    "3. Semantic Analysis\n",
    "4. Discourse Integration\n",
    "5. Pragmatic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Give an example of pragmatic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pragmatic is the fifth and last phase of NLP. It helps you to discover the intended effect by applying a set of rules that characterize cooperative dialogues.\n",
    "\n",
    "For Example: \"Open the door\" is interpreted as a request instead of an order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explain the morphological and lexical analysis procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphological Analysis:\n",
    "- In this analysis, we try to understand distinct words according to their morphemes, which are defined as the smallest units of meaning.\n",
    "\n",
    "For Example, Consider the word: “unhappiness ”\n",
    "\n",
    "- We can be broken down into three morphemes named prefix, stem, and suffix, with each conveying some form of meaning:\n",
    "\n",
    "    - The prefix un- refers to “not being”,\n",
    "\n",
    "    - The suffix -ness refers to “a state of being”.\n",
    "\n",
    "    - The stem happy is considered a free morpheme since it is a “word” on its own.\n",
    "\n",
    "- prefixes and suffixes are Bound morphemes and they require a free morpheme to which it can be attached, and can therefore not appear as a “word” on their own.\n",
    "\n",
    "#### Lexical Analysis:\n",
    "- This analysis involves identifying and analyzing the structure of words.\n",
    "\n",
    "- In a language, the Lexicon of a language describes the collection of words and phrases.\n",
    "\n",
    "- In Lexical analysis, we divide the whole chunk of text data into paragraphs, sentences, and words.\n",
    "\n",
    "- To work with lexical analysis, mostly we need to perform Lexicon Normalization. The most common lexicon normalization practices are Stemming and Lemmatization which we will cover later in this blog series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://www.analyticsvidhya.com/blog/2021/07/nltk-a-beginners-hands-on-guide-to-natural-language-processing/\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/part-10-step-by-step-guide-to-master-nlp-named-entity-recognition/#:~:text=Named%20Entity%20Recognition%20is%20one,classify%20them%20into%20predefined%20categories.\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/part-2-step-by-step-guide-to-master-natural-language-processing-nlp-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
