{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define Natural Language Processing (NLP).\n",
    "2. What is the acronym for NLP?\n",
    "3. Any four NLP applications should be mentioned.\n",
    "4. Explain the Spacy library in a few words.\n",
    "5. Mention how important NLTK is.\n",
    "6. Mention the NLTK library's libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data\" - Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the acronym for NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP Stands for Natural Langauge Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Any four NLP applications should be mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Speech Recognition:\n",
    "    - Speech Recognition is a technology that enables the computer to convert voice input data to machine readable format. There are a lot of fields where speech recognition is used like, virtual assistants, adding speech-to-text, translating speech, sending emails etc. \n",
    "    - Examples: Google Assistance, Cortana, Siri, etc.\n",
    "    \n",
    "    \n",
    "2. Voice Assistants and Chatbots:\n",
    "    - Voice assistant is a software that uses NLP and speech recognition to understand voice commands of a user and perform accordingly. Similarly, Chatbots are programs that are designed to assist an user 24/7 and respond appropriately and answer any query that the user might have.    \n",
    "    - Most Chatbots and Virtual Assistants have pre-programmed answering systems that follow specific rules and patterns while answering. Powerful AI has enabled some voice assistants to interact with the user and respond appropriately. With more usage, they even improve themselves. Assistants like Siri and Alexa can even have a conversation with the user like a normal human being!\n",
    "    - Examples: Alexa, Customer service Chat of Paytm/Amazon, etc.\n",
    "\n",
    "    \n",
    "3. Sentiment Analysis:\n",
    "    - Human speech could be quite hard to interpret as it involves expressions and sentiments beyond literal meanings. Expressions like sarcasm, threat, exclamation etc. are often very hard to be recognised by the computer. But, with the help of Natural Language Understanding (NLU) which is a subfield of Natural Language Processing, the machine is able to catch on to different sentiments that might be insisted on through the user’s command.  \n",
    "    - Through sentiment analysis, one can analyse customer reactions, handle social media disputes by eradicating negative comments and getting insights from the customer base of any business.\n",
    "    - Examples: Alexa, Customer service Chat of Paytm/Amazon, etc.\n",
    "\n",
    "    \n",
    "4. Email Filtering:\n",
    "    - Most of the professional work is done through emails and it would be quite a hassle if all the emails we received were not segregated into different sections. Gmail classifies all the emails into primary, social and promotional sections. Even all the spam emails are sent to a different section so that they do not flood our inbox. \n",
    "    - This is done with the help of text classification, which is a technique of NLP.  It has definitely helped us save time and not miss any important Email that might have gotten lost if all the useless emails started accumulating in our inbox.\n",
    "\n",
    "Source: https://www.analyticssteps.com/blogs/top-10-applications-natural-language-processing-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain the Spacy library in a few words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\n",
    "\n",
    "- spaCy is a free, open-source library for NLP in Python. It's written in Cython and is designed to build information extraction or natural language understanding systems. It's built for production use and provides a concise and user-friendly API.\n",
    "\n",
    "- Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage.spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mention how important NLTK is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The process of cleaning unstructured text data, so that it can be used to predict, analyze, and extract information. Real-world text data is unstructured, inconsistent. So, Data preprocessing becomes a necessary step.\n",
    "\n",
    "    - The various Data Preprocessing methods are:\n",
    "       - Tokenization\n",
    "       - Frequency Distribution of Words\n",
    "       - Filtering Stop Words\n",
    "       - Stemming\n",
    "       - Lemmatization\n",
    "       - Parts of Speech(POS) Tagging\n",
    "       - Name Entity Recognition\n",
    "       - WordNet\n",
    "    - These are some of the methods to process the text data in NLP. The list is not so exhaustive but serves as a great starting point for anyone who wants to get started with NLP.\n",
    "\n",
    "- NLTK (Natural Language Toolkit) is a really powerful tool to preprocess text data for further analysis like with ML models for instance. It helps convert text into numbers, which the model can then easily work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Mention the NLTK library's libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the libraries of NLTK:\n",
    "\n",
    "#### 1. Tokenization:\n",
    "The process of breaking down the text data into individual tokens(words, sentences, characters) is known as Tokenization. It is a foremost step in Text Analytics.\n",
    "\n",
    "<b>1.1 Sentence Tokenization:</b> When text data is split into sentences, then it is sentence tokenization. It helps when text data consists of multiple paragraphs.\n",
    "\n",
    "<b>1.2 Word Tokenization:</b> When text data is split into individual words, then it is word tokenization. It is implemented\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1: Sentence Tokenization:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them.\"\"\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text_to_sentence = sent_tokenize(text)\n",
    "text_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origional Sentence: In 3000 years of our history, people from all over \n",
      "               the world have come and invaded us, captured our lands, conquered our minds.\n",
      "output after word tokenization: ['In', '3000', 'years', 'of', 'our', 'history', ',', 'people', 'from', 'all', 'over', 'the', 'world', 'have', 'come', 'and', 'invaded', 'us', ',', 'captured', 'our', 'lands', ',', 'conquered', 'our', 'minds', '.']\n"
     ]
    }
   ],
   "source": [
    "# 1.2: Word Tokenization:\n",
    "sent1= text_to_sentence[1]\n",
    "print(\"Origional Sentence:\",sent1)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word = word_tokenize(sent1)\n",
    "print(\"output after word tokenization:\",tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Frequency Distribution of words: \n",
    "We can generate the frequency distribution of words in a text by using the FreqDist() function in NLTK. Even results can be plotted using the matplotlib plot() function.\n",
    "\n",
    "\n",
    "\n",
    "- most_common() is the function used to print the most frequent words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'our': 3, ',': 3, 'In': 1, '3000': 1, 'years': 1, 'of': 1, 'history': 1, 'people': 1, 'from': 1, 'all': 1, ...})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq_dist_of_words = FreqDist(tokenized_word)\n",
    "freq_dist_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('our', 3), (',', 3), ('In', 1), ('3000', 1), ('years', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using most_common() to print most frequent words\n",
    "freq_dist_of_words.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Filtering Stop Words: \n",
    "Stop words are used to filter some words which are repetitive and don’t hold any information. For example, words like – {that these, below, is, are, etc.} don’t provide any information, so they need to be removed from the text. Stop Words are considered as Noise. NLTK provides a huge list of stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 's', 'ourselves', 'i', 'few', \"wasn't\", 'him', 'having', 'all', 'am', \"you'd\", 'they', 'those', 'theirs', 'were', 'against', 'me', 'no', 'being', 'couldn', \"isn't\", 'this', 'does', 'do', 'o', 'before', 'up', 'of', 'or', 'with', 'didn', 'has', 'same', \"you've\", 'such', 'who', 'himself', 'it', 'other', 'but', 'your', 'when', 'ain', 'by', 'while', 'from', 'its', \"should've\", 'about', \"couldn't\", 'his', \"wouldn't\", 'he', 'is', 'again', 'some', 'don', 'their', 'doesn', \"aren't\", 'why', 'on', 'm', 'shan', 'wasn', \"you're\", 'into', 'be', 'yourself', 'weren', 'just', \"hasn't\", 'we', \"shan't\", 'ours', 'whom', 'y', 't', 'd', 'most', 'hadn', 'and', \"haven't\", 'should', 'down', 'once', 're', 'wouldn', \"that'll\", \"shouldn't\", 'through', 'any', 'below', \"didn't\", 'her', 'not', 'you', 'then', 'itself', 'was', 'an', 'until', \"needn't\", 'yourselves', 'did', 'as', 'more', \"mustn't\", 'there', 'can', \"hadn't\", 'under', 'she', 'both', 'these', 'because', 'off', 'further', 'nor', 'too', 'won', 'll', 'if', 'aren', \"doesn't\", 'doing', 'herself', 'hasn', 'only', 'own', 'shouldn', 'after', 'them', 'each', 'mustn', 'my', 'a', 've', 'will', \"she's\", 'are', 'than', 'between', 'hers', 'isn', 'had', 'what', \"don't\", 'yours', 'in', 'how', \"weren't\", \"you'll\", 'during', 'needn', 'out', 'our', 'to', 'that', \"mightn't\", \"won't\", 'where', 'the', \"it's\", 'mightn', 'very', 'have', 'over', 'here', 'now', 'so', 'above', 'been', 'myself', 'themselves', 'at', 'for', 'ma', 'which'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxenized list with stop words: ['Learn', 'to', 'lose', 'your', 'destiny', 'to', 'find', 'where', 'it', 'leads', 'you']\n",
      "Toxenized list with out stop words: ['Learn', 'lose', 'destiny', 'find', 'leads']\n"
     ]
    }
   ],
   "source": [
    "#Filtering Stop Words 2\n",
    "#Removing Stop words:\n",
    "\n",
    "text = 'Learn to lose your destiny to find where it leads you'\n",
    "filtered_text = []\n",
    "tokenized_word = word_tokenize(text)\n",
    "for each_word in tokenized_word:\n",
    "    if each_word not in stop_words:\n",
    "        filtered_text.append(each_word)\n",
    "print('Toxenized list with stop words: {}'.format(tokenized_word))\n",
    "print('Toxenized list with out stop words: {}'.format(filtered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming: \n",
    "Stemming is a process of normalization, in which words are reduced to their root word (or) stem. Spacy doesn’t support stemming, so we need to use the NLTK library.\n",
    "\n",
    "   - Types of stemming:\n",
    "        - Porter Stemmer\n",
    "        - Snowball Stemmer\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy--->happi\n",
      "happier--->happier\n",
      "happiest--->happiest\n",
      "happiness--->happi\n",
      "breathing--->breath\n",
      "fairly--->fairli\n",
      "eating--->eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pstemmer = PorterStemmer()\n",
    "words = ['happy', 'happier', 'happiest', 'happiness', 'breathing','fairly','eating']\n",
    "for word in words:\n",
    "    print(word + '--->' + pstemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "As we can see above, the words are reduced to their stem word, but one thing we can notice is that the porter stemmer is not giving good results. So, we will use the Snowball stemmer, which is better when compared with porter stemmer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy--->happi\n",
      "happier--->happier\n",
      "happiest--->happiest\n",
      "happiness--->happi\n",
      "breathing--->breath\n",
      "fairly--->fair\n",
      "eating--->eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem = SnowballStemmer(language='english')\n",
    "words = ['happy', 'happier', 'happiest', 'happiness', 'breathing','fairly','eating']\n",
    "for word in words:\n",
    "    print(word + '--->' + snow_stem.stem(word))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Lemmatization: \n",
    "Like stemming, lemmatization is also used to reduce the word to their root word. Lemmatizing gives the complete meaning of the word which makes sense. It uses vocabulary and morphological analysis to transform a word into a root word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Stop Words: ['Life', 'will', 'always', 'have', 'problems', 'and', 'pressures', '.']\n",
      "Lemmatized Words list ['Life', 'will', 'always', 'have', 'problem', 'and', 'pressure', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"Life will always have problems and pressures.\"\n",
    "lemmatized_words_list = []\n",
    "tokenized_word = word_tokenize(text)\n",
    "for each_word in tokenized_word:\n",
    "    lem_word = lemmatizer.lemmatize(each_word)\n",
    "    lemmatized_words_list.append(lem_word)\n",
    "print('Text with Stop Words: {}'.format(tokenized_word))\n",
    "print('Lemmatized Words list {}'.format(lemmatized_words_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Parts of Speech(pos) tagging:  \n",
    "Parts of Speech(POS) tagging is the process of identifying parts of speech of a sentence. It is used to identify nouns, verbs, adjectives, adverbs, etc., and tag each word. There are many tagging sets available in NLTK,  but I will be using Universal Tag Set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " (\"'m\", 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('meet', 'VERB'),\n",
       " ('M.S', 'NOUN'),\n",
       " ('.', '.'),\n",
       " ('Dhoni', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parts of Speech(pos) tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "text = \"I'm going to meet M.S. Dhoni.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "nltk.pos_tag(tokenized_word, tagset='universal')\n",
    "text = \"I'm going to meet M.S. Dhoni.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "nltk.pos_tag(tokenized_word, tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Named Entity Recognition: \n",
    "Named Entity Recognition is used to identify names of organizations, people, and geographic locations in the text and tag them to the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Bhavantik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Sundar/NOUN\n",
      "  Pichai/NOUN\n",
      "  ,/.\n",
      "  the/DET\n",
      "  CEO/NOUN\n",
      "  of/ADP\n",
      "  Google/NOUN\n",
      "  Inc./NOUN\n",
      "  is/VERB\n",
      "  walking/VERB\n",
      "  in/ADP\n",
      "  the/DET\n",
      "  streets/NOUN\n",
      "  of/ADP\n",
      "  (GPE California/NOUN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "text = \"Sundar Pichai, the CEO of Google Inc. is walking in the streets of California.\"\n",
    "tokenized_word = word_tokenize(text)\n",
    "tags = nltk.pos_tag(tokenized_word, tagset='universal')\n",
    "entities = nltk.chunk.ne_chunk(tags, binary=False)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. WordNet: \n",
    "WordNet is a huge collection of words with meanings just like a traditional dictionary, used to generate synonyms, antonyms of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('army_intelligence.n.01'), Synset('artificial_intelligence.n.01'), Synset('three-toed_sloth.n.01'), Synset('artificial_insemination.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonym = wordnet.synsets(\"AI\")\n",
    "print(synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the branch of computer science that deal with writing computer programs that can solve problems creatively\n"
     ]
    }
   ],
   "source": [
    "#Print definition using WordNet:\n",
    "print(synonym[1].definition()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['workers in AI hope to imitate or duplicate intelligence in computers and robots']\n"
     ]
    }
   ],
   "source": [
    "#Print examples using WordNet:\n",
    "\n",
    "print(synonym[1].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "1. https://www.analyticsvidhya.com/blog/2021/07/getting-started-with-nlp-using-nltk-library/\n",
    "2. https://towardsdatascience.com/intro-to-nltk-for-nlp-with-python-87da6670dde#:~:text=NLTK%20(Natural%20Language%20Toolkit)%20is,can%20then%20easily%20work%20with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
