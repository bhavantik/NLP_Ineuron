{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 4\n",
    "1. Mention the steps that go into resolving any NLP issue.\n",
    "2. Mention how important word2vec is.\n",
    "3. Explain the process of feature extraction in NLP.\n",
    "4. What is the distinction between precision and recall?\n",
    "5. Explain the concept of tokenization.\n",
    "6. Mention the distinction between formal and informal language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mention the steps that go into resolving any NLP issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belowe are the steps that helps into resolving any NLP issue:\n",
    "\n",
    "Step 1: Gather your data:\n",
    "- Every Machine Learning problem starts with data, such as a list of emails, posts, or tweets. \n",
    "Step 2: Clean your data:\n",
    "- Here is a checklist to use to clean your data: (see the code for more details):\n",
    "    - Remove all irrelevant characters such as any non alphanumeric characters\n",
    "    - Tokenize your text by separating it into individual words\n",
    "    - Remove words that are not relevant, such as “@” twitter mentions or urls\n",
    "    - Convert all characters to lowercase, in order to treat words such as “hello”, “Hello”, and “HELLO” the same\n",
    "    - Consider combining misspelled or alternately spelled words to a single representation (e.g. “cool”/”kewl”/”cooool”)\n",
    "    - Consider lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n",
    "\n",
    "Step 3: Find a good data representation:\n",
    "- Machine Learning models take numerical values as input. Models working on images, for example, take in a matrix representing the intensity of each pixel in each color channel.\n",
    "- Our dataset is a list of sentences, so in order for our algorithm to extract patterns from the data, we first need to find a way to represent it in a way that our algorithm can understand, i.e. as a list of numbers.\n",
    "\n",
    "Step 4: Classification:\n",
    "- When first approaching a problem, a general best practice is to start with the simplest tool that could solve the job. Whenever it comes to classifying data, a common favorite for its versatility and explainability is Logistic Regression. It is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model.\n",
    "\n",
    "Step 5: Inspection:\n",
    "- Confusion Matrix\n",
    "    - A first step is to understand the types of errors our model makes, and which kind of errors are least desirable.\n",
    "\n",
    "Step 6: Accounting for vocabulary structure:\n",
    "- TF-IDF\n",
    "    - In order to help our model focus more on meaningful words, we can use a TF-IDF score (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise.\n",
    "\n",
    "Step 7: Leveraging semantics:\n",
    "\n",
    "- Word2Vec\n",
    "    - It is very likely that if we deploy this model, we will encounter words that we have not seen in our training set before. even if it has seen very similar words during training.\n",
    "    - To solve this problem, we need to capture the semantic meaning of words, meaning we need to understand that words like ‘good’ and ‘positive’ are closer than ‘apricot’ and ‘continent.’ The tool we will use to help us capture meaning is called Word2Vec.\n",
    "    - Word2Vec is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts.\n",
    "\n",
    "Step 8: Leveraging syntax using end-to-end approaches:\n",
    "- By omitting the order of words, we are discarding all of the syntactic information of our sentences. If these methods do not provide sufficient results, you can utilize more complex model that take in whole sentences as input and predict labels without the need to build an intermediate representation. A common way to do that is to treat a sentence as a sequence of individual word vectors using either Word2Vec or more recent approaches such as GloVe or CoVe. \n",
    "\n",
    "- Convolutional Neural Networks for Sentence Classification train very quickly and work well as an entry level deep learning architecture. While Convolutional Neural Networks (CNN) are mainly known for their performance on image data, they have been providing excellent results on text related tasks, and are usually much quicker to train than most complex NLP approaches (e.g. LSTMs and Encoder/Decoder architectures). This model preserves the order of words and learns valuable information on which sequences of words are predictive of our target classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mention how important word2vec is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus.\n",
    "- The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. \n",
    "- Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention.\n",
    "- The key benefit of the approach is that high-quality word embeddings can be learned efficiently (low space and time complexity), allowing larger embeddings to be learned (more dimensions) from much larger corpora of text (billions of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explain the process of feature extraction in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Natural Language Processing, Feature Extraction is one of the trivial steps to be followed for a better understanding of the context of what we are dealing with. After the initial text is cleaned and normalized, we need to transform it into their features to be used for modeling.\n",
    "- We use some particular method to assign weights to particular words within our document before modeling them. We go for numerical representation for individual words as it’s easy for the computer to process numbers, in such cases, we go for word embeddings.\n",
    "\n",
    "- There are various ways to perform feature extraction. some popular and mostly used are:-\n",
    "    - Bag of Words model\n",
    "    - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words\n",
    "- Bag-of-words model is a commonly used document representation method in the field of information retrieval .\n",
    "- In information retrieval, the BOW model assumes that for a document, it ignores its word order, grammar, syntax and other factors, and treats it as a collection of several words. The appearance of each word in the document is independent and independent of whether other words appear. (It's out of order)\n",
    "- The Bag-of-words model (BoW model) ignores the grammar and word order of a text, and uses a set of unordered words to express a text or a document.\n",
    "\n",
    "####  Example\n",
    "\n",
    "    -> John likes to watch movies. Mary likes too.\n",
    "\n",
    "    -> John also likes to watch football games.\n",
    "\n",
    "- Build a dictionary based on the words that appear in the above two sentences:\n",
    "\n",
    "{\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\": 6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}\n",
    "\n",
    "\n",
    "- The dictionary contains 10 words, each word has a unique index. Note that their order is not related to the order in which they appear in the sentence. According to this dictionary, we re-express the above two sentences into the following two vectors:\n",
    "\n",
    "    -> [1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\n",
    "\n",
    "    -> [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "\n",
    "- These two vectors contain a total of 10 elements, where the i-th element represents the number of times the i-th word in the dictionary appears in the sentence. \n",
    "\n",
    "- Now imagine a **huge document set D with a total of M documents**. After all the words in the document are extracted, they form a dictionary containing N words. Using the Bag-of-words model, **each document can be represented as an N-dimensional vector**.\n",
    "\n",
    "\n",
    "- Therefore, the BoW model can be considered as a statistical histogram. It is used in text retrieval and processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF:\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency), a commonly used weighting technique for information retrieval and information exploration.\n",
    "\n",
    "- TF-IDF is a statistical method used to evaluate the importance of a word to a file set or a file in a corpus. The importance of the word increases in proportion to the number of times it appears in the file, but at the same time decreases inversely with the frequency of its appearance in the corpus.\n",
    "\n",
    "- **Term frequency TF (item frequency)**: number of times a given word appears in the text. This number is usually normalized (the numerator is generally smaller than the denominator) to prevent it from favoring long documents, because whether the term is important or not, it is likely to appear more often in long documents than in paragraph documents.\n",
    "    - **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
    "\n",
    "- Term frequency (TF) indicates how often a term (keyword) appears in the text .\n",
    "\n",
    "- This number is usually normalized (usually the word frequency divided by the total number of words in the article) to prevent it from favoring long documents.\n",
    "\n",
    "* **Inverse document frequency (IDF)**: A measure of the general importance of a word. The main idea is that if there are fewer documents containing the entry t and the larger, it means that the entry has a good ability to distinguish categories. The IDF of a specific word can be calculated by dividing the total number of files by the number of files containing the word, and then taking the log of the obtained quotient.\n",
    "     - **IDF(t) = log_e(Total number of documents / Number of documents with term t in it).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "Consider a document containing 100 words where in the word cat appears 3 times. \n",
    "\n",
    "- The term frequency (Tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these.\n",
    "\n",
    "- Then, the inverse document frequency (Idf) is calculated as log(10,000,000 / 1,000) = 4.\n",
    "\n",
    "Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What is the distinction between precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - For classification precision-recall tradeoff is important. \n",
    "   - Imbalanced classes occur commonly in datasets and when it comes to specific use cases, we would in fact like to give more importance to the precision and recall metrics, and also how to achieve the balance between them.\n",
    "    \n",
    "**Precision**:\n",
    " - Precision is the ratio between the True Positives and all the Positives.\n",
    " - Precision also gives us a measure of the relevant data points.\n",
    " \n",
    "**Recall**:\n",
    " - The recall is the measure of our model correctly identifying True Positives. \n",
    " - So, Recall is the ratio between the True Positives and True Positive + False Negative.\n",
    " - We refer to it as Sensitivity or True Positive Rate\n",
    " \n",
    " Difference:\n",
    " - Precision quantifies the number of positive class predictions that actually belong to the positive class. Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. \n",
    " - Precision and recall counter each other, that is, increasing one of them reduces the other.if you select almost everything, the precision is very low, while the recall is very high; if you select almost nothing, precision is very high, while the recall is very low. \n",
    " - In the context of automated grammar checking (i.e. automatic identification and classification of grammar errors), precision is a ratio between all adequately identified errors and all identified errors, whereas recall can be defined as a ratio of all identified errors and all existing errors in a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explain the concept of tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n",
    "- For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’\n",
    "- Tokenization can be done to either separate words or sentences. If the text is split into words using some separation technique it is called word tokenization and same separation done for sentences is called sentence tokenization.\n",
    "- There are various tokenization techniques available which can be applicable based on the language and purpose of modeling. Below are a few of the tokenization techniques used in NLP.\n",
    "    - White Space Tokenization\n",
    "    - Dictionary Based Tokenization\n",
    "    - Rule Based Tokenization\n",
    "    - Regular Expression Tokenizer\n",
    "    - Penn TreeBank Tokenization\n",
    "    - Spacy Tokenizer\n",
    "    - Moses Tokenizer\n",
    "    - Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The field of natural language processing began in the 1940s, after World War II.',\n",
       " 'At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically.',\n",
       " 'However, the task was obviously not as easy as people first imagined.',\n",
       " 'By 1958, some researchers were identifying significant issues in the development of NLP.',\n",
       " 'One of these researchers was Noam Chomsky, who found it troubling that models of language recognized sentences that were nonsense but grammatically correct as equally irrelevant as sentences that were nonsense and not grammatically correct.',\n",
       " 'Chomsky found it problematic that the sentence “Colorless green ideas sleep furiously” was classified as improbable to the same extent that “Furiously sleep ideas green colorless”; any speaker of English can recognize the former as grammatically correct and the latter as incorrect, and Chomsky felt the same should be expected of machine models.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "  \n",
    "text = \"The field of natural language processing began in the 1940s, after World War II. At this time, people recognized the importance of translation from one language to another and hoped to create a machine that could do this sort of translation automatically. However, the task was obviously not as easy as people first imagined. By 1958, some researchers were identifying significant issues in the development of NLP. One of these researchers was Noam Chomsky, who found it troubling that models of language recognized sentences that were nonsense but grammatically correct as equally irrelevant as sentences that were nonsense and not grammatically correct. Chomsky found it problematic that the sentence “Colorless green ideas sleep furiously” was classified as improbable to the same extent that “Furiously sleep ideas green colorless”; any speaker of English can recognize the former as grammatically correct and the latter as incorrect, and Chomsky felt the same should be expected of machine models.\"\n",
    "sent = sent_tokenize(text)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The field of natural language processing began in the 1940s, after World War II.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'field',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'began',\n",
       " 'in',\n",
       " 'the',\n",
       " '1940s',\n",
       " ',',\n",
       " 'after',\n",
       " 'World',\n",
       " 'War',\n",
       " 'II',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "words = word_tokenize(sent[0])\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mention the distinction between formal and informal language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formal and informal language serve different purposes in written communications depending on the reader (Audience) and reason for writing (Purpose). The tone, the choice of words and the way the words are put together vary between the two different styles.\n",
    "\n",
    "- Formal language is less personal than informal language. It is used when writing for professional or academic purposes like graduate school assignments. Formal language does not use colloquialisms, contractions or first-person pronouns such as “I” or “We.”\n",
    "\n",
    "- Informal language is more casual and spontaneous. It is used when communicating with friends or family either in writing or in conversation. It is used when writing personal emails, text messages and in some business correspondence. The tone of informal language is more personal than formal language.\n",
    "\n",
    "- Examples of formal and informal language are shown below:\n",
    "    - Informal:The improvements can’t be made due to budget cuts.\n",
    "    - Formal: Improvements cannot be made due to budget restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n",
    "- https://www.analyticsvidhya.com/blog/2021/07/feature-extraction-and-embeddings-in-nlp-a-beginners-guide-to-understand-natural-language-processing/#:~:text=In%20Natural%20Language%20Processing%2C%20Feature,to%20be%20used%20for%20modeling.\n",
    "- https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/\n",
    "- https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4#:~:text=Tokenization%20is%20breaking%20the%20raw,the%20sequence%20of%20the%20words.\n",
    "- https://www.touro.edu/departments/writing-center/tutorials/formal-vs-informal-language/#:~:text=Formal%20language%20is%20less%20personal,is%20more%20casual%20and%20spontaneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
