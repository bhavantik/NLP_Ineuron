{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 5\n",
    "1. Explain the concept of lemmatization.\n",
    "2. What exactly is NLU? Mention how it's used.\n",
    "3. What exactly are stop words?\n",
    "4. What is corpus juris?\n",
    "5. Describe the veiled language model.\n",
    "6. Explain how a word embedding works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain the concept of lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In stemming and in lemmatization, we try to reduce a given word to its root word. The root word is called a stem in the stemming process, and it is called a lemma in the lemmatization process.\n",
    "\n",
    "- In stemming, a part of the word is just chopped off at the tail end to arrive at the stem of the word. There are definitely different algorithms used to find out how many characters have to be chopped off, but the algorithms don’t actually know the meaning of the word in the language it belongs to. \n",
    "- In lemmatization, on the other hand, the algorithms have this knowledge. In fact, you can even say that these algorithms refer a dictionary to understand the meaning of the word before reducing it to its root word, or lemma.\n",
    "\n",
    "\n",
    "- So, a lemmatization algorithm would know that the word better is derived from the word good, and hence, the lemme is good. But a stemming algorithm wouldn’t be able to do the same. \n",
    "- There could be over-stemming or under-stemming, and the word better could be reduced to either bet, or bett, or just retained as better. But there is no way in stemming that it could be reduced to its root word good. This, basically is the difference between stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What exactly is NLU? Mention how it's used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural language understanding is a branch of artificial intelligence that uses computer software to understand input in the form of sentences using text or speech.\n",
    "- NLU enables human-computer interaction. It is the comprehension of human language such as English, Spanish and French, for example, that allows computers to understand commands without the formalized syntax of computer languages. NLU also enables computers to communicate back to humans in their own languages.\n",
    "- The main purpose of NLU is to create chat- and voice-enabled bots that can interact with the public without supervision. Many major IT companies, such as Amazon, Apple, Google and Microsoft, and startups have NLU projects underway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What exactly are stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stopwords are the words in any language which does not add much meaning to a sentence. \n",
    "- They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. \n",
    "- In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as “The Who” or “Take That”.\n",
    "\n",
    "- If we have a task of text classification or sentiment analysis then we should remove stop words as they do not provide any information to our model, i.e keeping out unwanted words out of our corpus, but if we have the task of language translation then stopwords are useful, as they have to be translated along with other words.\n",
    "\n",
    "- There is no hard and fast rule on when to remove stop words. But I would suggest removing stop words if our task to be performed is one of Language Classification, Spam Filtering, Caption Generation, Auto-Tag Generation, Sentiment analysis, or something that is related to text classification.\n",
    "\n",
    "- On the other hand, if our task is one of Machine Translation, Question-Answering problems, Text Summarization, Language Modeling, it’s better not to remove the stop words as they are a crucial part of these applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'three', 'visions', 'for', 'india', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               \"\"\"\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(sentences[0])\n",
    "# applying stopwards \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three', 'visions', 'india', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is corpus juris?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The legal term Corpus Juris means \"body of law\".\n",
    "\n",
    "- It was originally used by the Romans for several of their collections of all the laws in a certain field—see Corpus Juris Civilis—and was later adopted by medieval jurists in assembling the Corpus Juris Canonici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Describe the veiled language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  language model is a probability distribution over sequences of words. Given such a sequence of length m, a language model assigns a probability to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modelling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. \n",
    "\n",
    "- Essentially, the problem addressed by the mechanisms of the illustrative embodiments is that often times human beings have hidden, implied, or veiled meaning in the natural language that they utilize. As a result, natural language processing systems, which operate on the literal meaning of the terms and phrases found in natural language content, cannot identify this hidden, implied, or veiled meaning and instead may come to an incorrect conclusion as to what the natural language content is stating based on the literal meaning. \n",
    "\n",
    "- For example, announcements by organizational, governmental, and business leaders, politicians, spokespersons, public relations individuals, and the like, often utilize language that attempts to provide information while hiding or at least obscuring the real meaning behind the announcement by utilizing specific combinations of terms and phrases that are ambiguous in their meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explain how a word embedding works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features.\n",
    "\n",
    "- Word Embeddings are a method of extracting features out of text so that we can input those features into a machine learning model to work with text data. \n",
    "- They try to preserve syntactical and semantic information. The methods such as Bag of Words(BOW), CountVectorizer and TFIDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary.\n",
    "- We can get a sparse matrix if most of the elements are zero. \n",
    "- Large input vectors will mean a huge number of weights which will result in high computation required for training. Word Embeddings give a solution to these problems.\n",
    "\n",
    "\n",
    "There are two different approaches to get Word Embeddings:\n",
    "\n",
    "1) **Word2Vec:**\n",
    "- Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network.\n",
    "- Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW).\n",
    "    - Word2vec training format:\n",
    "\n",
    "<!--     Size Dimension\n",
    "\n",
    "    Word1 vector1\n",
    "    Word2 vector1\n",
    "    ....\n",
    "    WordN vectorN\n",
    " -->\n",
    "\n",
    "2) **GloVe:**\n",
    "- GloVe is an unsupervised learning algorithm for obtaining vocabulary vector representations. The aggregated global word co-occurrence statistics from the corpus are trained and the resulting representations show interesting linear substructures of the word vector space.\n",
    "\n",
    "- GloVe word vector format: GloVe is a type of Word embedding. The format of the GloVe word vector and word2vec is a little different from the Stanford open source code training. \n",
    "- The first line of the model trained by word2vec is: thesaurus size and dimensions, while gloVe does not\n",
    "\n",
    "    - GloVe training format:\n",
    "\n",
    "\n",
    "<!--     Word1 vector1\n",
    "    Word2 vector1\n",
    "    ....\n",
    "    WordN vectorN -->\n",
    "    \n",
    "- Therefore, we use the model trained by Glove to add a line of Vocabulary Size in front, and the model is used in the same way as word2vec. The official website provides a lot of word vector models trained using thesaurus, which can be downloaded and used directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "- https://www.techtarget.com/searchenterpriseai/definition/natural-language-understanding-NLU\n",
    "- https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47\n",
    "- https://patents.justia.com/patent/9760564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
